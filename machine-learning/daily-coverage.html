<DOCTYPE HTML>
<html>

<head>
  <title>Daily coverage</title>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX","output/HTML-CSS"],
      tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.2-latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
  <h1>Dail coverage</h1>
    <table border=1>
      <!--New Week-->
      <tr>
        <td rowspan=3>wk1</td>
        <td>M</td>
        <td>Syllabus</td>
      </tr>
      <tr>
        <td>W</td>
        <td>
          
          <p>Expected value. Densities and distributions (CDF).</p>
          
          <br>
          <div class="equation" lang="latex"> f(x) = F(x)' </div>
          </br>

          <p>Not all distributions have a density, but all densities have a distribution.</p>

        </td>
      </tr>
        <td>F</td>
        <td>

        <p>Continuing finding the error criteria. training MSE. Now we want to
        talk about, well, probably test MSE.</p>

        <p>No free lunch.</p>

        <div class="definition">
          consistent: the method converges to the truth asymptotically
        </div>

        <p>Are you trying to predict? Or are you trying to understand?</p>

        <div class="definition">
          overfit: 
        </div>

        <div class="definition">
           underfit: 
        </div>

        <div class="definition">
          flexible: more parameters, less interpretable
        </div>

        <p> What is the cost of adding parameters to a model? More degrees of
        freedom.</p>

        <div class="definition">
          Irreducible error: error in prediction, due to finite size of
          training data
        </div>

        <p> Want to avoid variability in the method. Variance and bias:
        mathematical definitions. There is a relationship between MSE, bias,
        and variance; you can get it by using the relation
        $Var(\hat{m}(x))=Var(\hat{m}(x)-m(x))$ </p>

        <p> For classification, there is a single best estimator, the Bayes
        estimator. But it can't be used because there are unknown parameters.
        There is a naive bayes classifier that is tries to estimate them.</p>

          <bf>

          <p>How to measure goodness of fit, mean sum of squares. Why square
          the residuals? Makes the math easier.</p>
        
        </td> </tr>


      
      <!--New Week-->
      <tr>
        <td rowspan=3>wk2</td>
        <td>M</td>
        <td>No class</td>
      </tr>
      <tr>
        <td>T</td>
        <td>
          <p>Bayes classifier, proved that it is the best possible discrete classifier</p>
          <p>K-means nearest neighbors.</p>
        </td>
      </tr>
      <tr>
        <td>F</td>
        <td>
          <p>Simple linear regression (based on linear regression notes by M)</p>

          <p>SLR assumes errors are on the Y, not the X, if you add in the X
          errors, things get tricky.</p>

          <p>Derive the solution to betas, we don't actually use this, we use
          the matrix solution. But the non-matrix one is easier to build off of
          ...</p>

          <p>Calculate bias of the beta estimators</p>

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>wk3</td>
        <td>
          M
        </td>
        <td>

            <p>Estimate the variance of e </p>

            <p>How to choose measurements of x to minimize variation in slope
            estimate</p>

            <p>estimated slope is an linear combination of the y's</p>

            <p>How to do this stuff in R</p>

        </td>
      </tr>
        <td>
          W
        </td>
        <td>
          strangely absent
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>

          <p>What if you fit a quadratic model to linear data? Test the
          significance of the quadratic coefficient. From this you can learn
          which parameters are significant but you cannot know, from this,
          which model is better for prediction.</p>

          <p>R-square, what does it mean, what is wrong with it? It only says
          how much of the data is explained by the model, it does not say which
          is better at predicting. R-square: How close is the model to the
          local constant? Locality matters, since a linear model fit across the
          arc of a parabola is still pretty good looking. See the examples in
          the notes. R-square is only good for linear relationships (think this
          through)? It really makes problems in higher dimensionality.</p>
          
          <p>It does not account for the complexity of the model. You can
          always find a model with an R-square of 0 for a given data set.</p>

          <p>Adjusted R-square: $R_{adj}(p)$, where $p$ is the number of
          parameters.</p>

          <p>F-test takes into account multiple testing. Why is it important?
          If it is not, then your model is totally wrong. Tests the hypotheses
          that all of the coefficients are 0. See notes. Don't worry about the
          details.</p>

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk3
        </td>
        <td>
          M
        </td>
        <td>
          sick miss, maybe covered L1 regression?
        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>
          <p>autocorrelation (cont)</p>

          <p>look at length of runs (runs.test in R), (greater than or less
          than the median?).</p>

          <p> if there is an autocorrelation, then everything in
          summary(model), except the estimates of the intercept and slope
          (which will still be unbiased), is wrong. The standard error </p>

          <p> if there is autocorrelation, we stop. Nothing in this course
          covers, need advanced methods</p>

          <p> test for unequal variance, we don't cover the details, bptest
          in R. R-square does not handle non-constant vairance well,
          underestimates quality of the fit. </p>

          <p>advice, don't do summary() first, first do residual plots, then
          do runs.test, then do plots of residuals. Basically, test each
          assumption you made before looking into the results. I think I
          could automate a lot of this.</p>

          <p> how to deal with outliers? plots work great in sufficiently low
          dimensions. Cooks distance, etc. What about outliers in the x?
          Leverage. L1 regression, helps if there are outliers in y, but does
          nothing for x outliers. Soemthing weird like median of squares will
          help, but we won't cover it.</p>
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>
          missed, sick
        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk3
        </td>
        <td>
          M
        </td>
        <td>

          <p>correlation between factors, the R 'car' package. What effect
          does correlation between factors have? How can we detect it? Always
          check for correlation. Colinearity. Multilinearity, tricky to figure out.</p>

          <p>confidence intervals, what exactly are they? compare to credible intervals.</p>

          <p>end of simple linear regression. oerview: what to do with the
          data: 1) make a linear model, 2) test assumptions, check residuals
          (are they a random sequence?), (normality test on covariates?),
          check for non-constant variance (non-constant variance test, this
          test assumes iid errors), now, if the previous ones hold, can do
          summary table. Check F. If standard error is larger than the
          estimate, probably means something is off, colinearity or such. If
          all this works, then you have a good model for explaining the data,
          though nott necessarily for prediction.</p>

          <p>moving on. what if y is {0,1}? If you try fitting this to a
          linear line, y will asymptotically go to (+-)infinity, not 0 and 1.
          Figure out the variance for the bernoulli, then sub in the linear
          model with betas and what not, see that the variance is dependent
          on x, this violates the assumption of linear models.</p>

          <p>So we need a function to map from R to (0,1). Possibilities,
          sigmoid function. There are others. </p>

        </td>
      </tr>
    </table>

    <h2>Notes of teaching style</h2>

    I saw for a moment the mismatch between minds. The student asks what
    complexity means in this context, "the dimensionality of the data". But the
    instructor misses this. The question seems trivial, of course complexity is
    the number of parameters in the model. But the student is confusing the
    complexity of the model and the complexity of the data.

</body>

</html>
