<DOCTYPE HTML>
<html>

<head>
  <title>Daily coverage</title>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX","output/HTML-CSS"],
      tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.2-latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
  <h1>Dail coverage</h1>
    <table border=1>
      <!--New Week-->
      <tr>
        <td rowspan=3>wk1</td>
        <td>M</td>
        <td>Syllabus</td>
      </tr>
      <tr>
        <td>W</td>
        <td>
          
          <p>Expected value. Densities and distributions (CDF).</p>
          
          <br>
          <div class="equation" lang="latex"> f(x) = F(x)' </div>
          </br>

          <p>Not all distributions have a density, but all densities have a distribution.</p>

        </td>
      </tr>
        <td>F</td>
        <td>

        <p>Continuing finding the error criteria. training MSE. Now we want to
        talk about, well, probably test MSE.</p>

        <p>No free lunch.</p>

        <div class="definition">
          consistent: the method converges to the truth asymptotically
        </div>

        <p>Are you trying to predict? Or are you trying to understand?</p>

        <div class="definition">
          overfit: 
        </div>

        <div class="definition">
           underfit: 
        </div>

        <div class="definition">
          flexible: more parameters, less interpretable
        </div>

        <p> What is the cost of adding parameters to a model? More degrees of
        freedom.</p>

        <div class="definition">
          Irreducible error: error in prediction, due to finite size of
          training data
        </div>

        <p> Want to avoid variability in the method. Variance and bias:
        mathematical definitions. There is a relationship between MSE, bias,
        and variance; you can get it by using the relation
        $Var(\hat{m}(x))=Var(\hat{m}(x)-m(x))$ </p>

        <p> For classification, there is a single best estimator, the Bayes
        estimator. But it can't be used because there are unknown parameters.
        There is a naive bayes classifier that is tries to estimate them.</p>

          <bf>

          <p>How to measure goodness of fit, mean sum of squares. Why square
          the residuals? Makes the math easier.</p>
        
        </td>
      </tr>


      
      <!--New Week-->
      <tr>
        <td rowspan=3>wk2</td>
        <td>M</td>
        <td>No class</td>
      </tr>
      <tr>
        <td>T</td>
        <td>
          <p>Bayes classifier, proved that it is the best possible discrete classifier</p>
          <p>K-means nearest neighbors.</p>
        </td>
      </tr>
      <tr>
        <td>F</td>
        <td>
          <p>Simple linear regression (based on linear regression notes by M)</p>

          <p>SLR assumes errors are on the Y, not the X, if you add in the X
          errors, things get tricky.</p>

          <p>Derive the solution to betas, we don't actually use this, we use
          the matrix solution. But the non-matrix one is easier to build off of
          ...</p>

          <p>Calculate bias of the beta estimators</p>

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>wk3</td>
        <td>
          M
        </td>
        <td>

            <p>Estimate the variance of e </p>

            <p>How to choose measurements of x to minimize variation in slope
            estimate</p>

            <p>estimated slope is an linear combination of the y's</p>

            <p>How to do this stuff in R</p>

        </td>
      </tr>
        <td>
          W
        </td>
        <td>
          strangely absent
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>

          <p>What if you fit a quadratic model to linear data? Test the
          significance of the quadratic coefficient. From this you can learn
          which parameters are significant but you cannot know, from this,
          which model is better for prediction.</p>

          <p>R-square, what does it mean, what is wrong with it? It only says
          how much of the data is explained by the model, it does not say which
          is better at predicting. R-square: How close is the model to the
          local constant? Locality matters, since a linear model fit across the
          arc of a parabola is still pretty good looking. See the examples in
          the notes. R-square is only good for linear relationships (think this
          through)? It really makes problems in higher dimensionality.</p>
          
          <p>It does not account for the complexity of the model. You can
          always find a model with an R-square of 0 for a given data set.</p>

          <p>Adjusted R-square: $R_{adj}(p)$, where $p$ is the number of
          parameters.</p>

          <p>F-test takes into account multiple testing. Why is it important?
          If it is not, then your model is totally wrong. Tests the hypotheses
          that all of the coefficients are 0. See notes. Don't worry about the
          details.</p>

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk4
        </td>
        <td>
          M
        </td>
        <td>
          sick miss, maybe covered L1 regression?
        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>
          <p>autocorrelation (cont)</p>

          <p>look at length of runs (runs.test in R), (greater than or less
          than the median?).</p>

          <p> if there is an autocorrelation, then everything in
          summary(model), except the estimates of the intercept and slope
          (which will still be unbiased), is wrong. The standard error </p>

          <p> if there is autocorrelation, we stop. Nothing in this course
          covers, need advanced methods</p>

          <p> test for unequal variance, we don't cover the details, bptest
          in R. R-square does not handle non-constant vairance well,
          underestimates quality of the fit. </p>

          <p>advice, don't do summary() first, first do residual plots, then
          do runs.test, then do plots of residuals. Basically, test each
          assumption you made before looking into the results. I think I
          could automate a lot of this.</p>

          <p> how to deal with outliers? plots work great in sufficiently low
          dimensions. Cooks distance, etc. What about outliers in the x?
          Leverage. L1 regression, helps if there are outliers in y, but does
          nothing for x outliers. Soemthing weird like median of squares will
          help, but we won't cover it.</p>
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>
          missed, sick
        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk5
        </td>
        <td>
          M
        </td>
        <td>

          <p>correlation between factors, the R 'car' package. What effect
          does correlation between factors have? How can we detect it? Always
          check for correlation. Colinearity. Multilinearity, tricky to figure out.</p>

          <p>confidence intervals, what exactly are they? compare to credible intervals.</p>

          <p>end of simple linear regression. oerview: what to do with the
          data: 1) make a linear model, 2) test assumptions, check residuals
          (are they a random sequence?), (normality test on covariates?),
          check for non-constant variance (non-constant variance test, this
          test assumes iid errors), now, if the previous ones hold, can do
          summary table. Check F. If standard error is larger than the
          estimate, probably means something is off, colinearity or such. If
          all this works, then you have a good model for explaining the data,
          though nott necessarily for prediction.</p>

          <p>moving on. what if y is {0,1}? If you try fitting this to a
          linear line, y will asymptotically go to (+-)infinity, not 0 and 1.
          Figure out the variance for the bernoulli, then sub in the linear
          model with betas and what not, see that the variance is dependent
          on x, this violates the assumption of linear models.</p>

          <p>So we need a function to map from R to (0,1). Possibilities,
          sigmoid function. There are others. </p>

        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>
          missed this day also
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>

          <p>yeah, ..., so I am totally lost now. Dropping immediately into
          maximum likelyhood. Something logistic.</p>

          <p>$l(\beta_0, \beta_1) = ... = \pi_i ...$</p>

          <p>Fischer information, where is the variance accounted for? 

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk6
        </td>
        <td>
          M
        </td>
        <td>

          <p>Discussing logistic regression classifiers. Plot the sigmoid,
          above, 50%, say, is positive. Interpret all the summary() output.
          What are the residuals? Complicated, won't cover, but deviance. Where
          is the F-test? R doesn't give you the F-test, but you can do it.
          Build the NULL model, glm(x~1, ...). Then do an anova test, with the
          two models (outputs of glm). Then look at deviance and the p-value.</p>

          <p> Confusionn matrix. (predicted-class, true-class) matrix. Bayes
          classifier minimizes error, but from the human point of view,
          sometimes one type of error is preferable to another. For example,
          reduce false-positives at the cost of increasing the total error. Can
          you generalize the confustion matrix to n-classifiers. Then there are
          A predicted as B errors. </p>

          <p> You can plot FP against TP, characteristic curve. Always make
          these plots, the area under the curve, using test data. Checks the
          variance of the predictor, not a test. </p>

          <p>When is logistic regression good, what assumptions are made?
          Needn't assume normality or equal variance.</p>

        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>

          <p>Linear discriminant analysis</p>

          <p> Bayes rule, how does this help us? Split the denominator into
          full probability (sum of conditional probabilities). Want to
          estimates the posterior distribution, need to maximize, so only
          things that depend on k matter for maximization purposes, so
          denominator doesn't matter. So, we have
          
          $posterior \propto prior * likelihood$

          <p>Assume the likelyhood is normal with constant variance ...<\p>

          <p>Now, since we are dealing with maxima, we can take a log, the max
          and min will stay the same. This simplifies down to an equation
          linear relative to x.</p>

          <p>If we also assume all classes have equal variance, then
          discrimination mean is simply $x = \frac{\mu_1 + \mu_2}{2}$<\p>

          <p>So if we make all these horrific assumptions, then we get a
          classfier that is dependent only on the class means. If these
          assumptions hold, this is the best possible classifier.</p>

        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>
          <p> Bayesian classifier multivariate case. </p>
        </td>
      </tr>
          
    <!--New Week-->
    <tr>
      <td rowspan=3>
        wk7
      </td>
      <td>
        M
      </td>

      <td>
        <p>What do you do first, test if they are classifiable, do they
        overlap, there is a test named xxx. Are the data normal? yes/no, are
        they of equal variance? Build this tree ...</p>
        
        <p>Now discussing QDA, in LDA, the covariance matrix was constant, thus
        could be ignored when calculating the maximum. In QDA, the covariance
        matrix is different in each class. Look in the notes for the math
        development, the book seems to be wrong. What is the test for
        multivariate normal variance? QDA is a generalization of LDA, so you
        could alwaus use it, and test whether the quadratic coefficients are
        significantly different from 0. Then QDA reduces to LDA.</p>

        <p>Since LDA is less flexible (estimates ~k parameters rather than
        ~k^2, where k is number of dimensions), variance is lower (algthough
        bias will be higher). How does this differ, or does it, from
        overfitting?</p>
        
        <p>QDA is also computationally musch slower greater O(n^2).</p>
        
        <p> If you can assume conditional independence (which does not follow
        from iid) then you can decompose the a d-dimensional problem to the
        product of d 1-dimentionsal problems. Naive bayes.</p>
      </td>
    </tr>
    <tr>
      <td>
        W
      </td>
      <td>

        <p> Continueing Bayes classifier, why assume conditional independence.
        Reduces the dimensionality fo the problem. Avoids the curse of
        dimensionality. Width of hypotenuse in n-dimensions is square(n). All
        things tend to the edges, as a shell asymptotically. Can't really do
        higher dimensional density estimates, not practiacally at least.</p>

        <p> Estimate the density, simplest approach is just a histogram, but it
        is discrete. You can take the derivative of the CDF, but what is this?
        Go back the the definition of a derivative $lim_{h to inf} = ...$, move
        to an estimator of f(x), then an indicator: $\propto sum I\{ -1 \le
        \frac{x-X_i}{h}\}. Well, see the notes (error in first step). I've done
        all this before, but his development of the theory behind it is
        awesome.</p>

      </td>
    </tr>
    <tr>
      <td>
        F
      </td>
      <td>

        <p> Is there an optimal smoothing function (kernel function)? Yes, but
        it doesn't make much difference. More important is selection of the
        smoothing parameter. There is a complicated equation for estimating the
        optimal smoothing function.</p>

        <p>Can you test for the norality of the predictors in the Naive Bayes
        framework? We previously assumed conditional independence, we assumed
        it, we can't easily test for it. Not much point in testing for
        normal?</p>

      </td>
    </tr>

    <!--New Week-->
    <tr>
      <tr>
        <td rowspan=3>
        wk8
        </td>
        <td>
        M
        </td>
        <td>

          <p>Chapter 5 - cross-validation versus resampling. Get concepts from
          this section, don't need too much theory. Most the the standard
          statistical stuff, F-values for regression, etc, works great to
          describe how well. But says nothing about prediction ability. A
          linear model can be overfit. Chapter 5 develops methods that prevent
          overfitting, prevent a method from "bending too much".</p>

          <p> Three methods </p>

          <p> Dedicated training and testing sets </p>

          <p> leave one out </p>

          <p> k-fold cross something or other </p>

          <p> all of these are estimates of the prediction ability. the
          leave-one-out is basically unbiased, but the variance is large. The
          k-fold corss is more biased, but has lower variance.</p>

          <p> Never use two-fold cross validation, too much variance. Also
          don't just come up with some k-fold setup just because it divides
          evenly. </p>

          <p> You have to estimate the model parameters, e.g. smoothing
          parameter and others. Don't thoughtlessly use rules of thumb. </p>

          <p> glm is nice for this, even if you are doing pure linear
          regression, because you can use cv.glm to do cross-validation. By
          setting the family to identity,  </p>

          <p> choosing which parameter count to use it not trivial, there are
          methods for comparing error bars ... </p>

        </td>
      </tr>
    
      <tr>
        <td>
        W
        </td>
        <td>
          <p> Why not ever go past 10-fold? The variance becomes too high? </p>    

          <p> For k-nearest neighbors, how do you chose a good k? We have a
          good metric for prediction, we can just try every k and find the one
          with the minimum prediction error. The function will be messy, not a
          neat convex output. </p>

          <p> Choosing the hyper-parameter, e.g. k, is the usually the most
          time intensize part of the problem. Hutchinson's trace algorithm.
          When not to use GCV? No matter how much the noise in the data, it
          always finds the same value. This is because you take the mean of the
          trace, rather than taking each into account. </p>

          <p> So if you can't calculate the variance of an estimator, what can
          we do? Obviously we can collect more data. But if this is not
          possible? Bootstrap. Sampling without replacement results in
          dependence between sampled elements.</p>
          
          <p>But bootstrap can fail, for example you cannot bootstrap the
          maximum of a dataset. Cannot work on minima, maxima, boundary
          problems, that sort of thing. There are other variants of
          bootstrapping. Parametric varieties.</p>

        </td>
      </tr>
    
      <tr>
        <td>
        F
        </td>
        <td>
          <p> Bootstraps can also be used to estimate distributions. </p>    

          <p> Bootstrap can also be used for regression. Bootstrap a sample,
          estimate coefficients, repeat, then estimate error on the variables.
          The advantage is that bs makes no assumptions about the data, e.g.
          allows error in x. </p>

          <p> Bootstrap can fail, e.g. predicting max of uniform. Can try a
          parametric bootstrap. </p>

          <p> Bootstrap assumes iid, it also fails on heteroskedacity in the
          data. Then it has to account for the generation process. There are
          ways to deal with this. </p>

        </td>
      </tr>
    </td>

    <!--New Week-->
    <tr>
      <tr>
        <td rowspan=3>
        wk9
        </td>
        <td>
        M
        </td>
        <td>

          <p> On to chapter 6. Variable selection. You want to reduce
          parameters because of 1) overfitting and 2) the curse of
          dimensionality. If youhave more variales than data, then you do not
          have a unique solution.</p>

          <p> So given a huge number of parameters and few data, how can you
          choose the best (or at least a useful) subset of the parameters? 1)
          select subset 2) shrink coefficient, but keep original variables 3)
          dimensionality reduction (e.g. PCA), do not keep original
          variables.</p>

          <p> How to select a good model? </p>

          <p> Why not choose the high order polynomial when a linear one fits?
          High variance results from the large number of coefficients that need
          to be estimated. </p>

          <p> What criteria to use? Prediction error. Cross validation works,
          but is expensive. </p>

          <p> Akaike, simple derivation (run through this). Akaike can be
          strongly biased under certain conditions. AICC, Akaike corrected
          accounts for this. BIC, similar deal, higher penalty for complexity.
          </p>

          <p> There is a difference between BIC and AICC, difference between
          being selection consistent and model consistent. Say we have a set of
          models, $M$. If a point is outside the model class, it finds the
          closest model in the set. BIC can go to whatever if the point is
          outside the class. If the point is in the model class, BIC converges
          to the real one, AICC not necessarily does. Given this, there are
          cases where you should not use BIC (figure this out).</p>

          <p> There are a bunch of others. E.g. PRESS, which is just
          leave-one-out cross-validation.</p>

          <p> No method guarantees a best set of variables? Naive solution, try
          all possible combinations, this will give you the best possible set
          of variables for this model. </p>

          <p> But variable selection and model selection are completely
          different problems. </p>

        </td>
      </tr>
    
      <tr>
        <td>
        W
        </td>
        <td>

          <p> How many variables do you need to select? Never use RSS, take the
          minimum in Cp, BIC, adjusted R-sq (or here, this is actually a
          maximum), or whatever. (R leaps package)</p>

          <p> What if they tie or disagree (the predictors)? You can use an
          approach that actually sees the data, cross validation (e.g. PRESS).
          Do full cross-validation on all the models would be computationally
          very expensive. </p>

          <p> To find the best model, `coef(x)` </p>

          <p> IF you have hundreds of variables, you can't use these
          combinatorial approaches. Forward selection, start at one variable,
          add them incrementally, ignoring if it doesn't improve the model,
          etc. Forward selection doesn't guarantee the best subset of
          variables.</p>

          `G = lm(...)`

          `press(update(G, - . + x))`

          <p> There is always a backwards algorithm, but it is not always
          possible. If the number of variables is greater than the number of
          data points. </p>

        </td>
        <td>
        F
        </td>
        <td>

          <p> How to invert a matrix that is not invertible, add something to
          the diagonal. Ridge regression, ... reduces variance, increases bias.
          </p>

        </td>
      </tr>

    <!--New Week-->
    <tr>
      <tr>
        <td rowspan=3>
        wk10
        </td>
        <td>
        M
        </td>
        <td>

          <p>Ridge regression. What happens when lambda is 0, infinity? Run
          through the proofs. Calculating lambda takes time, but ridge
          regression always obtains a lower error result. However, making
          confidence intervals is hard because ridge regression is biased. For
          an unbiased estimator of beta, the confidence interval reduces to
          t-distribution. But if biased, you have a complicated mess.</p>  

          <p>How does ordinary least squares relate to ridge regression?</p>

        </td>
      </tr>
    
      <tr>
        <td>
        W
        </td>
        <td>

          <p> Ridge regression versus lasso. Lasso sets to exactly 0. We
          aren't going into too much theory, just understand. If there is
          colinearity in the data, can we use lasso, does it make the matrix
          invertible. No, can't use lasso if there is colinearity, have to use
          ridge regression.</p>

          <p> What is an L1 norm? Lasso is L1, Ridge is L2. ?  Elastic nets are
          something inbetween. Alpha-1 is lasso, Alpha-2 is ridge, in the R
          function ... </p>

          <p> Don't forget to tune lambda, only use for linear models (there is
          a kernel version for non-linear). </p>

          <p> Ridge and lasso have a Bayesian interpretation. If the prior is
          normal, you have ridge. If Laplacian, then lasso. </p>

          <p> forward, backward, hybrid, lasso. Only these select components.
          Reduce the dimension. Others (e.g. ridge) just shrink the
          coefficients. </p>

          <p> PCA! Columns that contain little variation (i.e. information)
          aren't analytically usefull. </p>

          <p> Two steps to PCA, 1) shift the center point of the data to origin
          and then 2) rotate data so variation is maximized </p>

        </td>
      </tr>
    
      <tr>
        <td>
        F
        </td>
        <td>

          <p> Continuing PCA. Translation and then rotation. What is a
          non-convex problem? First step in PCA, you need to normalize to
          unitless data. Do PCA on the correlation matrix. </p>

          <p> Start with corr matrix R. get eigenvalue for R. Make plot of
          sorted eigenvalues, the spectrum, scree plot. </p>

          <p> PCA can fail on functions that are not ellipsoid? If it fails ... </p>

          <p> Where are the Ys? PCA is unsupervised. </p>

          <p> LASSO is usually more powerful now. </p>

          <p> Partial least squares, similar to PCA but builds in Y. Isn't all
          that great. It reduces, does regression, checks for improvement, if
          improvemed, add the component (based on cross-validation). Only the
          chemists really use it now. </p>

          <p> High dimensional space is lonely. </p>

          <p> How can we circumvent the curse? Model each dimentions
          seperately, then combine them. This works ONLY if we assume additive
          structure to the data. </p>

          <p> Rule of thumb: for each dimension have around 100 points. </p>

        </td>
      </tr>
    </td>

    <!--New Week-->
    <tr>
      <tr>
        <td rowspan=3>
        wk11
        </td>
        <td>
        M
        </td>
        <td>
          
          <p> How to find a function to describe arbitrary data. </p>

          <p> All non-parametric estimators suffer from boundary effects. At the edges you lose half the data. </p>

          <p> The problem with adapting the sizes of the windows is that you have to estimate the density. </p>

          <p> Best possible way to build a line through through the data, requires choosing an h. How to determine which is good? Cross-validatoin. </p>

        </td>
      </tr>
    
      <tr>
        <td>
        W
        </td>
        <td>

          <p> Never fit an even numbered polynomial, because the change from an
          even degree to an odd degree variance never changes. Variance
          increases when you change from odd to even degree. Local polynomial
          regression is super good, automatically handles the boundary problem.
          </p>

          <p> What are the advantages of local polyomial </p>

          <p> Smoothing regression vs curve fitting - curve fitting was made by
          mathematicians, they assume no noise. </p>

          <p> SPLINES: another non-parametric means of fitting. </p>

          <p> The spline chooses a poitn and cuts it in half, piecewise. Doing
          it in higher dimensions is pretty tricky. The spline areas do not
          overlap? This seems like a shit method.</p>

          <p> Cubic splines preferred because it is the smallest polynomial
          with a 2nd-derivative, and 2nd-derivative continuity is the smallest
          that the human eye can perceive. </p>

          <p> $4(K+1)$ parameters for the polynomials, but to smooth to 2nd
          derivative, adds 3 constraints, so: $4(K+1) - 3K = K + 4$ </p>

          <p> So, how many knots? Too many and you interpolate, too few and you
          get too much bias. </p>

          <p> Q: 1) how many knots, 2) where to put the knots, 3) </p>

        </td>
      </tr>
    
      <tr>
        <td>
        F
        </td>
        <td>
          <p> Continuing with Splines. </p>    

          <p> Choices: How many knots do you use? Where do you put them? </p>

          <p> At K=0, you have a global polynomial. Try each K, cross-validate. </p>

          <p> Natural cubic spline is cubic spline everywhere except at the
          boundaries, first and last knots, where it is linear. In this case we
          have K degrees of freedom, we lose 2 df at each end. This decreased
          flexibility reduces irregularities at the ends. </p>

          <p> Better method? Add in all knots. Add a penalty which is
          proportional to the integral of the squared 2nd derivative. What if
          lambda is infinite, then we are in linear regression. Smoothing
          spline. You will loose the integer number of degrees of freedom.</p>

        </td>
      </tr>
    </td>

      <!--New Week-->
      <tr>
        <tr>
          <td rowspan=3>
          wk12
          </td>
          <td>
          M
          </td>
          <td>
            <p> Smoothing spline is a natural 3rd order polynomial with a knots
            at every point. A penalty term is added (see above). Usually, choose
            a smoothing plain, but don't forget to tune the lambda. </p>

            <p> Extend to 2 dimensions? Thin-plate spline, not very easy to
            calculate, nor often used. </p>

            <p> You can circumvent the curse if you can express the function as a
            linear combination of funtions of a single variable. That is, if you
            assume an additive model. If this can be done, you can solve the
            system using backfit. </p>

            <p> For a huge nomuber of dimensions, KNN is not great (everything is
            too distance, maybe don't go over 5 or so). LR not great if the
            boundary is non-linear. </p>

            <p> In general, what model will work? Think through the assumptions
            the models make, think through the assumptions the problem allow you
            to make, then connect the dots. </p>

            <p> General additive models. </p>

            <p> For next week, support vector machines. Use the notes, not the
            book, for chapter 9 on support vecotor machines. </p>
          </td>
        </tr>
      
        <tr>
          <td>
          W
          </td>
          <td>

            <p> SVM - we want a convex problem (LR, QDA, LDA all are not convex).
            SVM based on only 3 points in the linearly seperable case. The normal
            vectors between the closest three points and the optimal line are the
            support vectors. 
            </p>

            <p> See notes at end of my notebook </p>

          </td>
        </tr>
      
        <tr>
          <td>
          F
          </td>
          <td>

            <p> Yesterday covered ideal case with linearly seperable classes. </p>

            <p> More details on the margins, why we scale the way we do.
            Remember, the margin is half the distance between the nearest two
            points. </p>

            <p> 2-norm? What is a 2-norm? The margin is a 2-norm (whatever that
            is). The 2-norm is t(X)X. So we get the thing we want to minimize,
            now, how do we solve it? Legrange multipliers (see notes on
            optimization). This is used to minimize something when there are
            constraints.</p>

            <p> You can solve for alpha and w, but not b. Solving b is a
            quadratic optimization problem. Nobody in 1961 wanted to deal with
            that. </p>
      
          </td> </tr> </td>

          <!--New Week-->
          <tr>
            <tr>
              <td rowspan=3>
              wk13
              </td>
              <td>
              M
              </td>
              <td>

                <p> SVM, only the support vectors matter. Rule: find minimum
                set of support vectors such that y less than 1. What if they
                overlap?  Then there is no solution, can't solve the quadratic
                problem.  Numeric optimization programs these days can still
                get an approximate answer. But from a strict SVM method, you
                get no solution? </p>
               
                <p> VC dimension, how many times can you shatter a set? Why
                don't statisticians like SVM? They can't prove the variance
                behavior, etc. Also he proved lower bounds using combinatoric
                approaches. </p>
                
                <p> Primal to Dual space. H -- sum alpha_i y_i x_i^T x + b.
                Sign of that equation is the class. </p>

                <p> How can you solve the problem for overlaping cases. </p>
                
              </td>
            </tr>
          
            <tr>
              <td>
              W
              </td>
              <td>

                <p> Continue suppport vectors with relaxed standards for
                mis-classification. Be sure to know what the constraints are (2
                of them). Solving requires Legrange multipliers, but don't know
                details. There is one tuning parameter. </p>

                <p> Not recommended for huge problem. SVM are quadratic since
                requires holding X^T in memory. There are smarter tricks that
                accelerates it, by taking two points at a time, checking
                constraints, then merging them to one point if they are met.
                </p>

                <p> Assumptions: none about X distribution, unlike LDA, but LR
                doesn't either, and is easily solvable. But if classes are
                non-linear, how to separate? LR can't. </p>

                <p> Neural nets were used prior to 1995 for non-linear
                problems, but there were very simple classes of problems that
                it could no classify. SVM solves this by transforming X into
                higher dimensions.The higher the dimension, the more likely the
                classes are to be seperable. </p>

                <p> Need a Hilbert space. </p>


              </td>
            </tr>
          
            <tr>
              <td>
              F
              </td>
              <td>

                <p> SVM, linear separation based on transforming into higher
                dimension where the classes are seperable. Transform only on
                the X's. Minimize xxx s.t. yyy is zzz. But now map x into a new
                space.</p> 

                <p> Function is positive definite, then the fourier transform
                is positive at all points. If positive semidefinite, some of
                the eigenvalues can be 0. So you don't actually need to
                transform, or even calculate phi. The kernel trick (has a
                theorem name) adds a new smoothing parameter that needs to be
                estimated.</p>

                <p> MLP kernels Mike does not recomend. </p>

                <p> Unbounded kernels are not good if you have outliers. </p>

                <p> RBF is gernerally pretty good, especially for continuous
                data (not as great for binary). </p>

                <p> SVM is actually a neural network. Neurons are kernels? SVM
                puts kernels only over data that are present in the dataset,
                and only over the support vectors. </p>

                <p> Simulated annealing at first then go to derivative free
                approach. Tends to go fairly fast. </p>

              </td>
            </tr>
          </td>

        <!--New Week-->
        <tr>
          <tr>
            <td rowspan=3>
            wk14
            </td>
            <td>
            M
            </td>
            <td>

              <p> How do we extend SVM to multiple classes? Just leave one
              class out, and run binary classification on class A versus
              classes non-A. Alternatively, do pairwise comparisions. </p>

              <p> SVM regression. Not used very much for regression because it
              doesn't deal well with sparseness, adds a third tuning parameter,
              sparseness. It is very important to set this </p>
              
              <p> How does each smoothing parameter affect the outcome, low
              sigma means closer fitting. Bound lead to ... ; epsilon can add
              bias by letting systematic trends fall inside the tube, too
              large, then everything is a support vector.</p>

              <p> The tube of accuracy, the epsilon around the equation, is NOT
              a confidence interval. Mike doesn't like it. Why, there can be
              structure inside the tube of accuracy that would be missed. </p>

              <p> Kernel ridge regression, no sparseness, no support vectors,
              not SVM anymore. Originally intended just to relax the
              restrictions. </p>

              <p> How you cross-validate affects the robustness of the SVM
              regression. It has a robust, l1, loss function, but if the
              corss-validation is not robust, neither will be the SVM. </p>

              <p> For non-parametric systems, every step must be robust or the
              end result is not. Loss function, cross-validation, etc. </p>

              <p> What does l1, l2 etc loss functions mean? I think this refers to  </p>

              <p> Do you have to tune parameters for each binary classification
              sub-problem in multi-class SVM? </p>

            </td>
          </tr>
        
          <tr>
            <td>
            W
            </td>
            <td>
              <p> R package for SVM, not as good as Matlab (no cross validation).
              lssvm, R package as a very good minimizing function for the
              smoothing parameters. Matlab, initlssvm, can leave smoothing
              parameters empty, it will figure them out. tunelssvm, tune
              parameters. </p>

              <p> Why is lssvm a smoothing. OFr cross-validation (min of 10 or sqrt(n)) </p>

              <p> For classification, lsssvm gives results nearly
              indistinguishable from standard svm. There is a difference in the
              regression setting. </p>

              <p> gridsearch versus simplex, Mike likes simplex. </p>

              <p> One-vs-one give complete bounds, one-vs-others can have spaces
              that the SVM can't classify. </p>

              <p> lsssvm and svm, which is robust? R has nothing for lssvm? </p>

              <p> Moving on to clustering, unsupervised learning. Problems, don't
              know the labels or how many there are.</p>

              <p> kmeans - assumptions - 1) no overlaps in classes 'c_i
              intersection c_j = null'. 2) all points can be classified, union of
              all classes includes all indices.</p>

              <p> We want to minimize the variance within groups and maximize
              variance between groups. </p>
            </td>
          </tr>
          <tr>
            <td>
            F
            </td>
            <td>
            
              <p> K Means Clustering. </p>

            </td>
          </tr>
        </td>

      <!--New Week-->
      <tr>
        <tr>
          <td rowspan=3>
          wk15
          </td>
          <td>
          M
          </td>
          <td>

            <p> First thing to do in KMC, find the number of clusters. When you
            get clusters, use gap_statistics. Find gap distances. </p>

            <p> If you want to get around the estimation of the number of
            clusters, you can use hiearchical clustering. However, the method
            you use to define distance is somewhat arbitrary. Also there is no
            guarantee that it will give an optimal solution. Then to define
            clusters, you drop a horizontal bar, ugly. </p>

          </td>
        </tr>
      
        <tr>
          <td>
          W
          </td>
          <td>

          <p> Distances, often you need to take into account correlation. In
          this case, you want to use the Mahabanois distance, which uses
          Euclidean distance weighted by variance. </p>

          <p> Chapter 8 - tree based methods. If done right super good, if not
          awful. Trees don't do feature selection, but the topology of the tree
          includes a type of relevance selection. Decisions are made on higher
          variables before lower variables are seen.</p>

          <p> Where to cut? Minimize RSS. </p>

          </td>
        </tr>
      
        <tr>
          <td>
          F
          </td>
          <td>

            <p> Continuing trees. Trees deal easily with categorical variables,
            unlike all others that we have seen, which require dummy variables.
            If you descend all the way down, then you can get overfitting. You
            need to prune the tree, add a penalty term which is a function of
            the number of leaves. Actually, just aT, where a is a constant
            chosen by cross-validation, and T is the number of leaves.</p>

            <p> Minimizing classification error does not work well for trees.
            Instead usually either Entropy or gini index is minimized. </p>

            <p> Gini index: minimize the variance over all the branches (sum of
            Bernuilli variances). </p>

            <p> Bagging. Bootstrap the data, build trees. But the trees are
            correlated. How to deal with that? </p>

            <p> Random forest: take subset of predictors in each tree (maybe
            sqrt(p)). About equal to SVM in performace. Loose the visual
            representation, but gain lower variance. </p>

            <p> Combining low variance methods the results is bad, but
            combining lots of weak learners, like individual trees, which have
            high variance, can be very effective. Very robust against
            overfitting.</p>

            <p> Boosting: grow the tre slowly, initialize to 0, take a first
            split, check the residuals. Take the residuals, then try to model
            the residuals. Can overfit by trying to learn from error.
            Eventually you will just memorize the data. So you must control,
            cross-validate. </p>

            <p> Boot strapping on average uses only 2/3 of all individuals, so
            you can test against that without doing extra cross-validation. Out
            of bad error. </p>

          </td>
        </tr>
      </td>

    </table>

    <!--

    <h2>Notes of teaching style</h2>

    <p>I saw for a moment the mismatch between minds. The student asks what
    complexity means in this context, "the dimensionality of the data". But the
    instructor misses this. The question seems trivial, of course complexity is
    the number of parameters in the model. But the student is confusing the
    complexity of the model and the complexity of the data.</p>

    <p>Why do people get lost in classes? What the instructor is saying at a given
    point is not complicated, but people are lost. To be fair I am lost.
    Reason? Because there are pieces of background missing. Context is missing.
    Understanding requires context.</p>

    <p> At the beginning of each lecture, you really ought to reintroduce every
    variable. </p>

    <h2>Questions</h2>

    How do you prove that an equation has no closed form solution?

    -->

</body>

</html>
