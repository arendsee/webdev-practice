<DOCTYPE HTML>
<html>

<head>
  <title>Daily coverage</title>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX","output/HTML-CSS"],
      tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.2-latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
  <h1>Dail coverage</h1>
    <table border=1>
      <!--New Week-->
      <tr>
        <td rowspan=3>wk1</td>
        <td>M</td>
        <td>Syllabus</td>
      </tr>
      <tr>
        <td>W</td>
        <td>
          
          <p>Expected value. Densities and distributions (CDF).</p>
          
          <br>
          <div class="equation" lang="latex"> f(x) = F(x)' </div>
          </br>

          <p>Not all distributions have a density, but all densities have a distribution.</p>

        </td>
      </tr>
        <td>F</td>
        <td>

        <p>Continuing finding the error criteria. training MSE. Now we want to
        talk about, well, probably test MSE.</p>

        <p>No free lunch.</p>

        <div class="definition">
          consistent: the method converges to the truth asymptotically
        </div>

        <p>Are you trying to predict? Or are you trying to understand?</p>

        <div class="definition">
          overfit: 
        </div>

        <div class="definition">
           underfit: 
        </div>

        <div class="definition">
          flexible: more parameters, less interpretable
        </div>

        <p> What is the cost of adding parameters to a model? More degrees of
        freedom.</p>

        <div class="definition">
          Irreducible error: error in prediction, due to finite size of
          training data
        </div>

        <p> Want to avoid variability in the method. Variance and bias:
        mathematical definitions. There is a relationship between MSE, bias,
        and variance; you can get it by using the relation
        $Var(\hat{m}(x))=Var(\hat{m}(x)-m(x))$ </p>

        <p> For classification, there is a single best estimator, the Bayes
        estimator. But it can't be used because there are unknown parameters.
        There is a naive bayes classifier that is tries to estimate them.</p>

          <bf>

          <p>How to measure goodness of fit, mean sum of squares. Why square
          the residuals? Makes the math easier.</p>
        
        </td>
      </tr>


      
      <!--New Week-->
      <tr>
        <td rowspan=3>wk2</td>
        <td>M</td>
        <td>No class</td>
      </tr>
      <tr>
        <td>T</td>
        <td>
          <p>Bayes classifier, proved that it is the best possible discrete classifier</p>
          <p>K-means nearest neighbors.</p>
        </td>
      </tr>
      <tr>
        <td>F</td>
        <td>
          <p>Simple linear regression (based on linear regression notes by M)</p>

          <p>SLR assumes errors are on the Y, not the X, if you add in the X
          errors, things get tricky.</p>

          <p>Derive the solution to betas, we don't actually use this, we use
          the matrix solution. But the non-matrix one is easier to build off of
          ...</p>

          <p>Calculate bias of the beta estimators</p>

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>wk3</td>
        <td>
          M
        </td>
        <td>

            <p>Estimate the variance of e </p>

            <p>How to choose measurements of x to minimize variation in slope
            estimate</p>

            <p>estimated slope is an linear combination of the y's</p>

            <p>How to do this stuff in R</p>

        </td>
      </tr>
        <td>
          W
        </td>
        <td>
          strangely absent
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>

          <p>What if you fit a quadratic model to linear data? Test the
          significance of the quadratic coefficient. From this you can learn
          which parameters are significant but you cannot know, from this,
          which model is better for prediction.</p>

          <p>R-square, what does it mean, what is wrong with it? It only says
          how much of the data is explained by the model, it does not say which
          is better at predicting. R-square: How close is the model to the
          local constant? Locality matters, since a linear model fit across the
          arc of a parabola is still pretty good looking. See the examples in
          the notes. R-square is only good for linear relationships (think this
          through)? It really makes problems in higher dimensionality.</p>
          
          <p>It does not account for the complexity of the model. You can
          always find a model with an R-square of 0 for a given data set.</p>

          <p>Adjusted R-square: $R_{adj}(p)$, where $p$ is the number of
          parameters.</p>

          <p>F-test takes into account multiple testing. Why is it important?
          If it is not, then your model is totally wrong. Tests the hypotheses
          that all of the coefficients are 0. See notes. Don't worry about the
          details.</p>

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk4
        </td>
        <td>
          M
        </td>
        <td>
          sick miss, maybe covered L1 regression?
        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>
          <p>autocorrelation (cont)</p>

          <p>look at length of runs (runs.test in R), (greater than or less
          than the median?).</p>

          <p> if there is an autocorrelation, then everything in
          summary(model), except the estimates of the intercept and slope
          (which will still be unbiased), is wrong. The standard error </p>

          <p> if there is autocorrelation, we stop. Nothing in this course
          covers, need advanced methods</p>

          <p> test for unequal variance, we don't cover the details, bptest
          in R. R-square does not handle non-constant vairance well,
          underestimates quality of the fit. </p>

          <p>advice, don't do summary() first, first do residual plots, then
          do runs.test, then do plots of residuals. Basically, test each
          assumption you made before looking into the results. I think I
          could automate a lot of this.</p>

          <p> how to deal with outliers? plots work great in sufficiently low
          dimensions. Cooks distance, etc. What about outliers in the x?
          Leverage. L1 regression, helps if there are outliers in y, but does
          nothing for x outliers. Soemthing weird like median of squares will
          help, but we won't cover it.</p>
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>
          missed, sick
        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk5
        </td>
        <td>
          M
        </td>
        <td>

          <p>correlation between factors, the R 'car' package. What effect
          does correlation between factors have? How can we detect it? Always
          check for correlation. Colinearity. Multilinearity, tricky to figure out.</p>

          <p>confidence intervals, what exactly are they? compare to credible intervals.</p>

          <p>end of simple linear regression. oerview: what to do with the
          data: 1) make a linear model, 2) test assumptions, check residuals
          (are they a random sequence?), (normality test on covariates?),
          check for non-constant variance (non-constant variance test, this
          test assumes iid errors), now, if the previous ones hold, can do
          summary table. Check F. If standard error is larger than the
          estimate, probably means something is off, colinearity or such. If
          all this works, then you have a good model for explaining the data,
          though nott necessarily for prediction.</p>

          <p>moving on. what if y is {0,1}? If you try fitting this to a
          linear line, y will asymptotically go to (+-)infinity, not 0 and 1.
          Figure out the variance for the bernoulli, then sub in the linear
          model with betas and what not, see that the variance is dependent
          on x, this violates the assumption of linear models.</p>

          <p>So we need a function to map from R to (0,1). Possibilities,
          sigmoid function. There are others. </p>

        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>
          missed this day also
        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>

          <p>yeah, ..., so I am totally lost now. Dropping immediately into
          maximum likelyhood. Something logistic.</p>

          <p>$l(\beta_0, \beta_1) = ... = \pi_i ...$</p>

          <p>Fischer information, where is the variance accounted for? 

        </td>
      </tr>

      <!--New Week-->
      <tr>
        <td rowspan=3>
          wk6
        </td>
        <td>
          M
        </td>
        <td>

          <p>Discussing logistic regression classifiers. Plot the sigmoid,
          above, 50%, say, is positive. Interpret all the summary() output.
          What are the residuals? Complicated, won't cover, but deviance. Where
          is the F-test? R doesn't give you the F-test, but you can do it.
          Build the NULL model, glm(x~1, ...). Then do an anova test, with the
          two models (outputs of glm). Then look at deviance and the p-value.</p>

          <p> Confusionn matrix. (predicted-class, true-class) matrix. Bayes
          classifier minimizes error, but from the human point of view,
          sometimes one type of error is preferable to another. For example,
          reduce false-positives at the cost of increasing the total error. Can
          you generalize the confustion matrix to n-classifiers. Then there are
          A predicted as B errors. </p>

          <p> You can plot FP against TP, characteristic curve. Always make
          these plots, the area under the curve, using test data. Checks the
          variance of the predictor, not a test. </p>

          <p>When is logistic regression good, what assumptions are made?
          Needn't assume normality or equal variance.</p>

        </td>
      </tr>
      <tr>
        <td>
          W
        </td>
        <td>

          <p>Linear discriminant analysis</p>

          <p> Bayes rule, how does this help us? Split the denominator into
          full probability (sum of conditional probabilities). Want to
          estimates the posterior distribution, need to maximize, so only
          things that depend on k matter for maximization purposes, so
          denominator doesn't matter. So, we have
          
          $posterior \propto prior * likelihood$

          <p>Assume the likelyhood is normal with constant variance ...<\p>

          <p>Now, since we are dealing with maxima, we can take a log, the max
          and min will stay the same. This simplifies down to an equation
          linear relative to x.</p>

          <p>If we also assume all classes have equal variance, then
          discrimination mean is simply $x = \frac{\mu_1 + \mu_2}{2}$<\p>

          <p>So if we make all these horrific assumptions, then we get a
          classfier that is dependent only on the class means. If these
          assumptions hold, this is the best possible classifier.</p>

        </td>
      </tr>
      <tr>
        <td>
          F
        </td>
        <td>
          <p> Bayesian classifier multivariate case. </p>
        </td>
      </tr>
          
    <!--New Week-->
    <tr>
      <td rowspan=3>
        wk7
      </td>
      <td>
        M
      </td>

      <td>
        <p>What do you do first, test if they are classifiable, do they
        overlap, there is a test named xxx. Are the data normal? yes/no, are
        they of equal variance? Build this tree ...</p>
        
        <p>Now discussing QDA, in LDA, the covariance matrix was constant, thus
        could be ignored when calculating the maximum. In QDA, the covariance
        matrix is different in each class. Look in the notes for the math
        development, the book seems to be wrong. What is the test for
        multivariate normal variance? QDA is a generalization of LDA, so you
        could alwaus use it, and test whether the quadratic coefficients are
        significantly different from 0. Then QDA reduces to LDA.</p>

        <p>Since LDA is less flexible (estimates ~k parameters rather than
        ~k^2, where k is number of dimensions), variance is lower (algthough
        bias will be higher). How does this differ, or does it, from
        overfitting?</p>
        
        <p>QDA is also computationally musch slower greater O(n^2).</p>
        
        <p> If you can assume conditional independence (which does not follow
        from iid) then you can decompose the a d-dimensional problem to the
        product of d 1-dimentionsal problems. Naive bayes.</p>
      </td>
    </tr>
    <tr>
      <td>
        W
      </td>
      <td>

        <p> Continueing Bayes classifier, why assume conditional independence.
        Reduces the dimensionality fo the problem. Avoids the curse of
        dimensionality. Width of hypotenuse in n-dimensions is square(n). All
        things tend to the edges, as a shell asymptotically. Can't really do
        higher dimensional density estimates, not practiacally at least.</p>

        <p> Estimate the density, simplest approach is just a histogram, but it
        is discrete. You can take the derivative of the CDF, but what is this?
        Go back the the definition of a derivative $lim_{h to inf} = ...$, move
        to an estimator of f(x), then an indicator: $\propto sum I\{ -1 \le
        \frac{x-X_i}{h}\}. Well, see the notes (error in first step). I've done
        all this before, but his development of the theory behind it is
        awesome.</p>

      </td>
    </tr>
    <tr>
      <td>
        F
      </td>
      <td>

        <p> Is there an optimal smoothing function (kernel function)? Yes, but
        it doesn't make much difference. More important is selection of the
        smoothing parameter. There is a complicated equation for estimating the
        optimal smoothing function.</p>

        <p>Can you test for the norality of the predictors in the Naive Bayes
        framework? We previously assumed conditional independence, we assumed
        it, we can't easily test for it. Not much point in testing for
        normal?</p>

      </td>
    </tr>

    <!--New Week-->
    <tr>
      <tr>
        <td rowspan=3>
        wk3
        </td>
        <td>
        M
        </td>
        <td>

          <p>Chapter 5 - cross-validation versus resampling. Get concepts from
          this section, don't need too much theory. Most the the standard
          statistical stuff, F-values for regression, etc, works great to
          describe how well. But says nothing about prediction ability. A
          linear model can be overfit. Chapter 5 develops methods that prevent
          overfitting, prevent a method from "bending too much".</p>

          <p> Three methods </p>

          <p> Dedicated training and testing sets </p>

          <p> leave one out </p>

          <p> k-fold cross something or other </p>

          <p> all of these are estimates of the prediction ability. the
          leave-one-out is basically unbiased, but the variance is large. The
          k-fold corss is more biased, but has lower variance.</p>

          <p> Never use two-fold cross validation, too much variance. Also
          don't just come up with some k-fold setup just because it divides
          evenly. </p>

          <p> You have to estimate the model parameters, e.g. smoothing
          parameter and others. Don't thoughtlessly use rules of thumb. </p>

          <p> glm is nice for this, even if you are doing pure linear
          regression, because you can use cv.glm to do cross-validation. By
          setting the family to identity,  </p>

          <p> choosing which parameter count to use it not trivial, there are
          methods for comparing error bars ... </p>

        </td>
      </tr>
    
      <tr>
        <td>
        W
        </td>
        <td>
          <p> Why not ever go past 10-fold? The variance becomes too high? </p>    

          <p> For k-nearest neighbors, how do you chose a good k? We have a
          good metric for prediction, we can just try every k and find the one
          with the minimum prediction error. The function will be messy, not a
          neat convex output. </p>

          <p> Choosing the hyper-parameter, e.g. k, is the usually the most
          time intensize part of the problem. Hutchinson's trace algorithm.
          When not to use GCV? No matter how much the noise in the data, it
          always finds the same value. This is because you take the mean of the
          trace, rather than taking each into account. </p>

          <p> So if you can't calculate the variance of an estimator, what can
          we do? Obviously we can collect more data. But if this is not
          possible? Bootstrap. Sampling without replacement results in
          dependence between sampled elements.</p>
          
          <p>But bootstrap can fail, for example you cannot bootstrap the
          maximum of a dataset. Cannot work on minima, maxima, boundary
          problems, that sort of thing. There are other variants of
          bootstrapping. Parametric varieties.</p>

        </td>
      </tr>
    
      <tr>
        <td>
        F
        </td>
        <td>
          <p> Bootstraps can also be used to estimate distributions. </p>    

          <p> Bootstrap can also be used for regression. Bootstrap a sample,
          estimate coefficients, repeat, then estimate error on the variables.
          The advantage is that bs makes no assumptions about the data, e.g.
          allows error in x. </p>

          <p> Bootstrap can fail, e.g. predicting max of uniform. Can try a
          parametric bootstrap. </p>

          <p> Bootstrap assumes iid, it also fails on heteroskedacity in the
          data. Then it has to account for the generation process. There are
          ways to deal with this. </p>

        </td>
      </tr>
    </td>

    <!--New Week-->
    <tr>
      <tr>
        <td rowspan=3>
        wk3
        </td>
        <td>
        M
        </td>
        <td>

          <p> On to chapter 6. Variable selection. You want to reduce
          parameters because of 1) overfitting and 2) the curse of
          dimensionality. If youhave more variales than data, then you do not
          have a unique solution.</p>

          <p> So given a huge number of parameters and few data, how can you
          choose the best (or at least a useful) subset of the parameters? 1)
          select subset 2) shrink coefficient, but keep original variables 3)
          dimensionality reduction (e.g. PCA), do not keep original
          variables.</p>

          <p> How to select a good model? </p>

          <p> Why not choose the high order polynomial when a linear one fits?
          High variance results from the large number of coefficients that need
          to be estimated. </p>

          <p> What criteria to use? Prediction error. Cross validation works,
          but is expensive. </p>

          <p> Akaike, simple derivation (run through this). Akaike can be
          strongly biased under certain conditions. AICC, Akaike corrected
          accounts for this. BIC, similar deal, higher penalty for complexity.
          </p>

          <p> There is a difference between BIC and AICC, difference between
          being selection consistent and model consistent. Say we have a set of
          models, $M$. If a point is outside the model class, it finds the
          closest model in the set. BIC can go to whatever if the point is
          outside the class. If the point is in the model class, BIC converges
          to the real one, AICC not necessarily does. Given this, there are
          cases where you should not use BIC (figure this out).</p>

          <p> There are a bunch of others. E.g. PRESS, which is just
          leave-one-out cross-validation.</p>

          <p> No method guarantees a best set of variables? Naive solution, try
          all possible combinations, this will give you the best possible set
          of variables for this model. </p>

          <p> But variable selection and model selection are completely
          different problems. </p>

        </td>
      </tr>
    
      <tr>
        <td>
        W
        </td>
    
        <td>
      </tr>
    
      <tr>
        <td>
        F
        </td>
    
        <td>
      </tr>
    </td>

    </table>

    <h2>Notes of teaching style</h2>

    <p>I saw for a moment the mismatch between minds. The student asks what
    complexity means in this context, "the dimensionality of the data". But the
    instructor misses this. The question seems trivial, of course complexity is
    the number of parameters in the model. But the student is confusing the
    complexity of the model and the complexity of the data.</p>

    <p>Why do people get lost in classes? What the instructor is saying at a given
    point is not complicated, but people are lost. To be fair I am lost.
    Reason? Because there are pieces of background missing. Context is missing.
    Understanding requires context.</p>

    <h2>Questions</h2>

    How do you prove that an equation has no closed form solution?

</body>

</html>
